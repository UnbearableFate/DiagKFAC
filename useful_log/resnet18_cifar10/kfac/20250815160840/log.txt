Model Type           : ResNet
Total Parameters     : 11,173,962
Trainable Parameters : 11,173,962
Device               : cuda
Use AMP              : True
Loss Function        : CrossEntropyLoss
Optimizer            : AdamW
Preconditioner       : KFACPreconditioner(
  accumulation_steps=1,
  allreduce_bucket_cap_mb=25,
  allreduce_method=AllreduceMethod.ALLREDUCE_BUCKETED,
  assignment=KAISAAssignment,
  assignment_strategy=AssignmentStrategy.COMPUTE,
  colocate_factors=True,
  compute_eigenvalue_outer_product=False,
  compute_method=ComputeMethod.EIGEN,
  damping=0.003,
  distributed_strategy=DistributedStrategy.COMM_OPT,
  factor_decay=0.95,
  factor_dtype=None,
  factor_update_steps=10,
  grad_scaler=True,
  grad_worker_fraction=1.0,
  inv_dtype=torch.float32,
  inv_update_steps=100,
  kl_clip=0.002,
  layers=41,
  loglevel=10,
  lr=<function Trainer.init_preconditioner.<locals>.<lambda> at 0x400159d8cea0>,
  skip_layers=[],
  steps=0,
  symmetry_aware=False,
  update_factors_in_hook=True,
)
  - Learning Rate    : 0.001
  - Weight Decay     : 0.001
Scheduler            : OneCycleLR
Batch Size           : 256
Epochs               : 43
Distributed          : True
EMA Model Used       : No
Output Directory     : ./out
Dataset Size         : 50000
Number of Classes    : 10
World Size           : 8
Rank                 : 0
============================================================

==== Arguments ====
data_path: /work/xg24i002/x10041/data/cifar10
model: resnet18Cifar
device: cuda
batch_size: 256
epochs: 43
workers: 8
opt: adamw
lr: 0.001
momentum: 0.9
weight_decay: 0.001
norm_weight_decay: 0.0
bias_weight_decay: 0.0
transformer_embedding_decay: 0.0
label_smoothing: 0.1
mixup_alpha: 0.8
cutmix_alpha: 1.0
lr_scheduler: onecycle
lr_warmup_epochs: 0
lr_warmup_method: constant
lr_warmup_decay: 0.01
lr_step_size: 30
lr_gamma: 0.1
pct_start: 0.2
lr_min: 0.0
print_freq: 100
output_dir: ./out
resume: 
start_epoch: 0
cache_dataset: False
sync_bn: False
test_only: False
auto_augment: ta_wide
ra_magnitude: 9
augmix_severity: 3
random_erase: 0.25
amp: True
world_size: 1
dist_url: env://
model_ema: False
model_ema_steps: 32
model_ema_decay: 0.99998
use_deterministic_algorithms: False
interpolation: bicubic
val_resize_size: 224
val_crop_size: 224
train_crop_size: 224
clip_grad_norm: 5.0
ra_sampler: False
ra_reps: 3
weights: None
backend: pil
use_v2: False
log_dir: ./logs/torch_cifar10
checkpoint_format: checkpoint_{epoch}.pth.tar
no_cuda: False
seed: 42
val_batch_size: 128
batches_per_allreduce: 1
checkpoint_freq: 10
kfac_inv_update_steps: 100
kfac_factor_update_steps: 10
kfac_update_steps_alpha: 10
kfac_update_steps_decay: None
kfac_inv_method: False
kfac_factor_decay: 0.95
kfac_damping: 0.003
kfac_damping_alpha: 0.5
kfac_damping_decay: None
kfac_kl_clip: 0.002
kfac_skip_layers: []
kfac_colocate_factors: True
kfac_strategy: comm-opt
kfac_grad_worker_fraction: 0.25
local_rank: 0
timestamp: 20250815160840
experiment_name: kfac
recover: False
preconditioner: kfac
degree_noniid: 0
dataset: cifar10
backpack: False
workspace_path: /work/xg24i002/x10041/DiagKFAC
distributed: True
